{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculates the average sentence length, average grammatical complexity, and average sentiment\n",
    "    score for each row of text in the input DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (Pandas DataFrame): A DataFrame with a single column of text\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame: A new DataFrame with the columns 'average_sentence_length', 'average_grammatical_complexity',\n",
    "        and 'average_sentiment'\n",
    "    \"\"\"\n",
    "    # Define a function to calculate the POS tag count for a given sentence\n",
    "    def pos_tag_count(sentence):\n",
    "        pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        tag_count = len(pos_tags)\n",
    "        return tag_count\n",
    "    \n",
    "    # Define a function to calculate the sentiment score for a given sentence\n",
    "    def sentiment_score(sentence):\n",
    "        blob = TextBlob(sentence)\n",
    "        score = blob.sentiment.polarity\n",
    "        return score\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = df['prompt'].apply(nltk.sent_tokenize)\n",
    "    \n",
    "    # Calculate the average sentence length for each row\n",
    "    df['average_sentence_length'] = sentences.apply(lambda x: sum(len(sentence.split()) for sentence in x)/len(x))\n",
    "    \n",
    "    # Calculate the average POS tag count for each row\n",
    "    df['average_grammatical_complexity'] = sentences.apply(lambda x: sum(pos_tag_count(sentence) for sentence in x)/len(x))\n",
    "    \n",
    "    # Calculate the average sentiment score for each row\n",
    "    df['average_sentiment'] = sentences.apply(lambda x: sum(sentiment_score(sentence) for sentence in x)/len(x))\n",
    "    \n",
    "    # Drop the 'sentences' column\n",
    "    #df = df.drop('sentences', axis=1)\n",
    "    \n",
    "    return df[['average_sentence_length', 'average_grammatical_complexity', 'average_sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"3class.xlsx\")\n",
    "\n",
    "metrics = calculate_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>completion</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>average_grammatical_complexity</th>\n",
       "      <th>average_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>college</td>\n",
       "      <td>24.437500</td>\n",
       "      <td>28.062500</td>\n",
       "      <td>0.287285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>college</td>\n",
       "      <td>15.057143</td>\n",
       "      <td>16.628571</td>\n",
       "      <td>0.072698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>college</td>\n",
       "      <td>24.045455</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.274590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10thgrade</td>\n",
       "      <td>14.978723</td>\n",
       "      <td>17.234043</td>\n",
       "      <td>0.107646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6thgrade</td>\n",
       "      <td>20.409091</td>\n",
       "      <td>24.136364</td>\n",
       "      <td>0.207507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>college</td>\n",
       "      <td>21.138889</td>\n",
       "      <td>23.777778</td>\n",
       "      <td>0.191063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>college</td>\n",
       "      <td>19.421053</td>\n",
       "      <td>21.789474</td>\n",
       "      <td>0.125174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>6thgrade</td>\n",
       "      <td>19.434783</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.342120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>6thgrade</td>\n",
       "      <td>20.236842</td>\n",
       "      <td>22.710526</td>\n",
       "      <td>0.070582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>6thgrade</td>\n",
       "      <td>20.428571</td>\n",
       "      <td>22.666667</td>\n",
       "      <td>0.080238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    completion  average_sentence_length  average_grammatical_complexity  \\\n",
       "0      college                24.437500                       28.062500   \n",
       "1      college                15.057143                       16.628571   \n",
       "2      college                24.045455                       27.500000   \n",
       "3    10thgrade                14.978723                       17.234043   \n",
       "4     6thgrade                20.409091                       24.136364   \n",
       "..         ...                      ...                             ...   \n",
       "295    college                21.138889                       23.777778   \n",
       "296    college                19.421053                       21.789474   \n",
       "297   6thgrade                19.434783                       21.000000   \n",
       "298   6thgrade                20.236842                       22.710526   \n",
       "299   6thgrade                20.428571                       22.666667   \n",
       "\n",
       "     average_sentiment  \n",
       "0             0.287285  \n",
       "1             0.072698  \n",
       "2             0.274590  \n",
       "3             0.107646  \n",
       "4             0.207507  \n",
       "..                 ...  \n",
       "295           0.191063  \n",
       "296           0.125174  \n",
       "297           0.342120  \n",
       "298           0.070582  \n",
       "299           0.080238  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test for average_sentence_length: chi-squared=38.000, p=0.0000000056\n",
      "Friedman test for average_grammatical_complexity: chi-squared=41.060, p=0.0000000012\n",
      "Friedman test for average_sentiment: chi-squared=7.740, p=0.0208583694\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "#df = pd.read_csv('data.csv')\n",
    "\n",
    "# Define your variables\n",
    "completion = 'completion'\n",
    "metrics = ['average_sentence_length', 'average_grammatical_complexity', 'average_sentiment']\n",
    "\n",
    "# Perform the Friedman test for each metric\n",
    "for metric in metrics:\n",
    "    data = []\n",
    "    for level in df[completion].unique():\n",
    "        data.append(df[df[completion]==level][metric].values)\n",
    "    stat, p = friedmanchisquare(*data)\n",
    "    print(f'Friedman test for {metric}: chi-squared={stat:.3f}, p={p:.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel(\"human_ai.xlsx\")\n",
    "\n",
    "metrics2 = calculate_metrics(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>average_grammatical_complexity</th>\n",
       "      <th>average_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nThe NBA is home to some of the most amazin...</td>\n",
       "      <td>ai</td>\n",
       "      <td>17.975610</td>\n",
       "      <td>20.878049</td>\n",
       "      <td>0.180748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evelyn McHale’s Most Beautiful Suicide\\n\\nA ph...</td>\n",
       "      <td>human</td>\n",
       "      <td>16.428571</td>\n",
       "      <td>18.968254</td>\n",
       "      <td>0.107971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nIn a world where technology has become an ...</td>\n",
       "      <td>ai</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>21.638889</td>\n",
       "      <td>0.184185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Analysis of the Role of Media Coverage of t...</td>\n",
       "      <td>human</td>\n",
       "      <td>39.333333</td>\n",
       "      <td>43.166667</td>\n",
       "      <td>-0.009496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nThe Familial Blackmail Of A Popular Senato...</td>\n",
       "      <td>ai</td>\n",
       "      <td>16.292683</td>\n",
       "      <td>18.097561</td>\n",
       "      <td>0.072050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>\\n\\nIn today's ever-evolving world, communicat...</td>\n",
       "      <td>ai</td>\n",
       "      <td>20.485714</td>\n",
       "      <td>23.685714</td>\n",
       "      <td>0.084739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>\\n\\nThe study of Anglo-Saxon culture and histo...</td>\n",
       "      <td>ai</td>\n",
       "      <td>23.457143</td>\n",
       "      <td>25.971429</td>\n",
       "      <td>0.093564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>\\n\\nA Vacation to Mars: The Biggest Scam in Mo...</td>\n",
       "      <td>ai</td>\n",
       "      <td>21.029412</td>\n",
       "      <td>23.235294</td>\n",
       "      <td>0.024079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>\\n\\nThe Winter Goddess\\n\\nWinter is one of the...</td>\n",
       "      <td>ai</td>\n",
       "      <td>19.828571</td>\n",
       "      <td>22.028571</td>\n",
       "      <td>0.081576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>\\n\\nOn the night of September 7, 2021, the Uni...</td>\n",
       "      <td>ai</td>\n",
       "      <td>23.647059</td>\n",
       "      <td>26.382353</td>\n",
       "      <td>0.236768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt completion  \\\n",
       "0    \\n\\nThe NBA is home to some of the most amazin...         ai   \n",
       "1    Evelyn McHale’s Most Beautiful Suicide\\n\\nA ph...      human   \n",
       "2    \\n\\nIn a world where technology has become an ...         ai   \n",
       "3    An Analysis of the Role of Media Coverage of t...      human   \n",
       "4    \\n\\nThe Familial Blackmail Of A Popular Senato...         ai   \n",
       "..                                                 ...        ...   \n",
       "445  \\n\\nIn today's ever-evolving world, communicat...         ai   \n",
       "446  \\n\\nThe study of Anglo-Saxon culture and histo...         ai   \n",
       "447  \\n\\nA Vacation to Mars: The Biggest Scam in Mo...         ai   \n",
       "448  \\n\\nThe Winter Goddess\\n\\nWinter is one of the...         ai   \n",
       "449  \\n\\nOn the night of September 7, 2021, the Uni...         ai   \n",
       "\n",
       "     average_sentence_length  average_grammatical_complexity  \\\n",
       "0                  17.975610                       20.878049   \n",
       "1                  16.428571                       18.968254   \n",
       "2                  19.000000                       21.638889   \n",
       "3                  39.333333                       43.166667   \n",
       "4                  16.292683                       18.097561   \n",
       "..                       ...                             ...   \n",
       "445                20.485714                       23.685714   \n",
       "446                23.457143                       25.971429   \n",
       "447                21.029412                       23.235294   \n",
       "448                19.828571                       22.028571   \n",
       "449                23.647059                       26.382353   \n",
       "\n",
       "     average_sentiment  \n",
       "0             0.180748  \n",
       "1             0.107971  \n",
       "2             0.184185  \n",
       "3            -0.009496  \n",
       "4             0.072050  \n",
       "..                 ...  \n",
       "445           0.084739  \n",
       "446           0.093564  \n",
       "447           0.024079  \n",
       "448           0.081576  \n",
       "449           0.236768  \n",
       "\n",
       "[450 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic for average_sentence_length: 2.3814547798227776\n",
      "P-value for average_sentence_length: 0.021173470428243146\n",
      "T-statistic for caverage_grammatical_complexity: 2.8082946812703815\n",
      "P-value for average_grammatical_complexity: 0.007132242100184873\n",
      "T-statistic for average_sentiment: -2.567474969176779\n",
      "P-value for average_sentiment: 0.013346831895158473\n"
     ]
    }
   ],
   "source": [
    "# Get 50 random entries from each category\n",
    "human_data = df2[df2['completion'] == 'human'].sample(n=50, random_state=42)\n",
    "ai_data = df2[df2['completion'] == 'ai'].sample(n=50, random_state=42)\n",
    "\n",
    "# Combine the two dataframes\n",
    "df_new = pd.concat([human_data, ai_data])\n",
    "\n",
    "# Perform dependent samples t-test between categories 1 and 2\n",
    "t_stat1, p_val1 = stats.ttest_rel(df_new[df_new['completion'] == 'human']['average_sentence_length'], \n",
    "                                  df_new[df_new['completion'] == 'ai']['average_sentence_length'])\n",
    "\n",
    "# Perform dependent samples t-test between categories 1 and 3\n",
    "t_stat2, p_val2 = stats.ttest_rel(df_new[df_new['completion'] == 'human']['average_grammatical_complexity'], \n",
    "                                  df_new[df_new['completion'] == 'ai']['average_grammatical_complexity'])\n",
    "\n",
    "# Perform dependent samples t-test between categories 2 and 3\n",
    "t_stat3, p_val3 = stats.ttest_rel(df_new[df_new['completion'] == 'human']['average_sentiment'], \n",
    "                                  df_new[df_new['completion'] == 'ai']['average_sentiment'])\n",
    "\n",
    "# Print results\n",
    "print('T-statistic for average_sentence_length:', t_stat1)\n",
    "print('P-value for average_sentence_length:', p_val1)\n",
    "print('T-statistic for caverage_grammatical_complexity:', t_stat2)\n",
    "print('P-value for average_grammatical_complexity:', p_val2)\n",
    "print('T-statistic for average_sentiment:', t_stat3)\n",
    "print('P-value for average_sentiment:', p_val3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
