{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"human_ai.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aaronweiss/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculates the average sentence length, average grammatical complexity, and average sentiment\n",
    "    score for each row of text in the input DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (Pandas DataFrame): A DataFrame with a single column of text\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame: A new DataFrame with the columns 'average_sentence_length', 'average_grammatical_complexity',\n",
    "        and 'average_sentiment'\n",
    "    \"\"\"\n",
    "    # Define a function to calculate the POS tag count for a given sentence\n",
    "    def pos_tag_count(sentence):\n",
    "        pos_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        tag_count = len(pos_tags)\n",
    "        return tag_count\n",
    "    \n",
    "    # Define a function to calculate the sentiment score for a given sentence\n",
    "    def sentiment_score(sentence):\n",
    "        blob = TextBlob(sentence)\n",
    "        score = blob.sentiment.polarity\n",
    "        return score\n",
    "    \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = df['prompt'].apply(nltk.sent_tokenize)\n",
    "    \n",
    "    # Calculate the average sentence length for each row\n",
    "    df['average_sentence_length'] = sentences.apply(lambda x: sum(len(sentence.split()) for sentence in x)/len(x))\n",
    "    \n",
    "    # Calculate the average POS tag count for each row\n",
    "    df['average_grammatical_complexity'] = sentences.apply(lambda x: sum(pos_tag_count(sentence) for sentence in x)/len(x))\n",
    "    \n",
    "    # Calculate the average sentiment score for each row\n",
    "    df['average_sentiment'] = sentences.apply(lambda x: sum(sentiment_score(sentence) for sentence in x)/len(x))\n",
    "    \n",
    "    # Drop the 'sentences' column\n",
    "    #df = df.drop('sentences', axis=1)\n",
    "    \n",
    "    return df[['average_sentence_length', 'average_grammatical_complexity', 'average_sentiment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nIn the late 1800s, a woman by the name of ...</td>\n",
       "      <td>10thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nThe Hunt for Enemy Technology - Early Jets...</td>\n",
       "      <td>10thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Division of Labour\\n\\nThe division of labo...</td>\n",
       "      <td>6thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n“Yes the body:” A Quarantined Review of Sp...</td>\n",
       "      <td>10thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nOnce upon a time, in a world far different...</td>\n",
       "      <td>6thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>\\n\\nThe History of Silicon Valley — A Brief Su...</td>\n",
       "      <td>6thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>\\n\\nYes the Body: A Quarantined Review of Spaw...</td>\n",
       "      <td>6thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>\\n\\nAs I stand atop the Eildon Hills, overlook...</td>\n",
       "      <td>college</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>\\n\\nAs human beings, we have always been fasci...</td>\n",
       "      <td>10thgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>\\n\\nRushing the Growler: A Risky Business\\n\\nT...</td>\n",
       "      <td>college</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt completion\n",
       "0    \\n\\nIn the late 1800s, a woman by the name of ...  10thgrade\n",
       "1    \\n\\nThe Hunt for Enemy Technology - Early Jets...  10thgrade\n",
       "2    The Division of Labour\\n\\nThe division of labo...   6thgrade\n",
       "3    \\n\\n“Yes the body:” A Quarantined Review of Sp...  10thgrade\n",
       "4    \\n\\nOnce upon a time, in a world far different...   6thgrade\n",
       "..                                                 ...        ...\n",
       "295  \\n\\nThe History of Silicon Valley — A Brief Su...   6thgrade\n",
       "296  \\n\\nYes the Body: A Quarantined Review of Spaw...   6thgrade\n",
       "297  \\n\\nAs I stand atop the Eildon Hills, overlook...    college\n",
       "298  \\n\\nAs human beings, we have always been fasci...  10thgrade\n",
       "299  \\n\\nRushing the Growler: A Risky Business\\n\\nT...    college\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df2 = pd.DataFrame()\n",
    "\n",
    "df2 = pd.read_excel(\"3class.xlsx\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt', 'completion', 'average_sentence_length', 'average_grammatical_complexity', 'average_sentiment']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the metrics\n",
    "metrics_df2 = calculate_metrics(df2)\n",
    "df2\n",
    "\n",
    "print(list(df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m X_text \u001b[39m=\u001b[39m pad_sequences(sequences, maxlen\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39m# Get the numerical data\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m X_num \u001b[39m=\u001b[39m data[[\u001b[39m'\u001b[39m\u001b[39maverage_sentence_length\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_grammatical_complexity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maverage_sentiment\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     36\u001b[0m \u001b[39m# Convert the labels to one-hot encoding\u001b[39;00m\n\u001b[1;32m     37\u001b[0m y \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(df[\u001b[39m'\u001b[39m\u001b[39mcompletion\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the text input\n",
    "text_input = Input(shape=(100,), name='text_input')\n",
    "embedding = Embedding(10000, 16, input_length=100)(text_input)\n",
    "pooling = GlobalAveragePooling1D()(embedding)\n",
    "\n",
    "# Define the numerical input\n",
    "num_input = Input(shape=(3,), name='num_input')\n",
    "\n",
    "# Concatenate the text and numerical inputs\n",
    "concat = Concatenate()([pooling, num_input])\n",
    "\n",
    "# Add a dense layer and output layer\n",
    "dense1 = Dense(16, activation='relu')(concat)\n",
    "output = Dense(3, activation='softmax')(dense1)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[text_input, num_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load the data\n",
    "# ...\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df2['prompt'])\n",
    "sequences = tokenizer.texts_to_sequences(df2['prompt'])\n",
    "X_text = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Get the numerical data\n",
    "X_num = df2[['average_sentence_length', 'average_grammatical_complexity', 'average_sentiment']].values\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(df2['completion'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_text_train, X_text_val, X_num_train, X_num_val, y_train, y_val = train_test_split(X_text, X_num, y, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([X_text_train, X_num_train], y_train, epochs=10, validation_data=([X_text_val, X_num_val], y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate([X_text_test, X_num_test], y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
